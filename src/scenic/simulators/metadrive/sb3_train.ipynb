{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931f5618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kxu/ScenicGym/src/scenic/core/errors.py:271: UserWarning: unable to install sys.excepthook to format Scenic backtraces\n",
      "  warnings.warn(\"unable to install sys.excepthook to format Scenic backtraces\")\n"
     ]
    }
   ],
   "source": [
    "from scenic.zoo import ScenicZooEnv\n",
    "from scenic.simulators.metadrive import MetaDriveSimulator\n",
    "import scenic\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "import supersuit as ss\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12b34e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kxu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_user = os.path.expanduser(\"~\")\n",
    "root_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca76cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumo_map = root_user + \"/ScenicGym/assets/maps/CARLA/Town01.net.xml\"\n",
    "obs_space_dict = {\"agent0\" :  gym.spaces.Box(-0.0, 1.0 , (249,), dtype=np.float32),\n",
    "                 \"agent1\": gym.spaces.Box(-0.0, 1.0 , (249,), dtype=np.float32)}\n",
    "\n",
    "action_space_dict = {'agent0': gym.spaces.Box(-1.0, 1.0, (2,), np.float32),\n",
    "                     'agent1': gym.spaces.Box(-1.0, 1.0, (2,), np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8622c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(need_monitor=False):\n",
    "    scenario = scenic.scenarioFromFile(\"intersect_drive.scenic\",\n",
    "                                   model=\"scenic.simulators.metadrive.model\",\n",
    "                               mode2D=True)\n",
    "\n",
    "    env = ScenicZooEnv(scenario, \n",
    "                       MetaDriveSimulator(sumo_map=sumo_map, render=False, real_time=False),\n",
    "                       None, \n",
    "                       max_steps=50, \n",
    "                       observation_space = obs_space_dict, \n",
    "                       action_space = action_space_dict, \n",
    "                       agents=[\"agent0\", \"agent1\"])\n",
    "    #if need_monitor:\n",
    "    #    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
    "    # Train a single model to play as each agent in an AEC environment\n",
    "    env = env_fn()\n",
    "\n",
    "    # Add black death wrapper so the number of agents stays constant\n",
    "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
    "    env = ss.black_death_v3(env)\n",
    "\n",
    "    # Pre-process using SuperSuit\n",
    "#     visual_observation = not env.unwrapped.vector_state\n",
    "#     if visual_observation:\n",
    "#         # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "#         env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "#         env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "#         env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "    # Use a CNN policy if the observation space is visual\n",
    "    model = PPO(\n",
    "        CnnPolicy if visual_observation else MlpPolicy,\n",
    "        env,\n",
    "        verbose=3,\n",
    "        batch_size=256,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e984fdfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENTS: ['agent0', 'agent1']\n",
      "AGENTS: ['agent0', 'agent1']\n",
      "black agents: ['agent0', 'agent1']\n",
      "OBSS: {'agent0': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.30923638,\n",
      "       0.30387354, 0.2988948 , 0.29427534, 0.29650518, 0.31154627,\n",
      "       0.32843173, 0.3475295 , 0.36928037, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        ], dtype=float32), 'agent1': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.14012794, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.34792858, 0.34149873,\n",
      "       0.3356267 , 0.33012754, 0.3250214 , 0.32022992, 0.31591237,\n",
      "       0.3118599 , 0.30811837, 0.3046719 , 0.32048067, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        ], dtype=float32)}\n",
      "Starting training on scenic_zoo_env_v0.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [4], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env_fn, steps, seed, **env_kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(env\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m env \u001b[38;5;241m=\u001b[39m ss\u001b[38;5;241m.\u001b[39mpettingzoo_env_to_vec_env_v1(env)\n\u001b[0;32m---> 38\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_vec_envs_v1\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstable_baselines3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Use a CNN policy if the observation space is visual\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m     42\u001b[0m     CnnPolicy \u001b[38;5;28;01mif\u001b[39;00m visual_observation \u001b[38;5;28;01melse\u001b[39;00m MlpPolicy,\n\u001b[1;32m     43\u001b[0m     env,\n\u001b[1;32m     44\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     45\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/SuperSuit/supersuit/vector/vector_constructors.py:64\u001b[0m, in \u001b[0;36mconcat_vec_envs_v1\u001b[0;34m(vec_env, num_vec_envs, num_cpus, base_class)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat_vec_envs_v1\u001b[39m(vec_env, num_vec_envs, num_cpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, base_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     63\u001b[0m     num_cpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_cpus, num_vec_envs)\n\u001b[0;32m---> 64\u001b[0m     vec_env \u001b[38;5;241m=\u001b[39m \u001b[43mMakeCPUAsyncConstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvec_env_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_vec_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m vec_env\n",
      "File \u001b[0;32m~/SuperSuit/supersuit/vector/concat_vec_env.py:23\u001b[0m, in \u001b[0;36mConcatVecEnv.__init__\u001b[0;34m(self, vec_env_fns, obs_space, act_space)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vec_env_fns, obs_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, act_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvec_envs \u001b[38;5;241m=\u001b[39m vec_envs \u001b[38;5;241m=\u001b[39m [vec_env_fn() \u001b[38;5;28;01mfor\u001b[39;00m vec_env_fn \u001b[38;5;129;01min\u001b[39;00m vec_env_fns]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vec_envs)):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(vec_envs[i], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_envs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/SuperSuit/supersuit/vector/concat_vec_env.py:23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vec_env_fns, obs_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, act_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvec_envs \u001b[38;5;241m=\u001b[39m vec_envs \u001b[38;5;241m=\u001b[39m [\u001b[43mvec_env_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m vec_env_fn \u001b[38;5;129;01min\u001b[39;00m vec_env_fns]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vec_envs)):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(vec_envs[i], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_envs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/SuperSuit/supersuit/vector/vector_constructors.py:12\u001b[0m, in \u001b[0;36mvec_env_args.<locals>.env_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menv_fn\u001b[39m():\n\u001b[0;32m---> 12\u001b[0m     env_copy \u001b[38;5;241m=\u001b[39m cloudpickle\u001b[38;5;241m.\u001b[39mloads(\u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_copy\n",
      "File \u001b[0;32m~/mambaforge/envs/scenicgym/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:62\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     61\u001b[0m     cp \u001b[38;5;241m=\u001b[39m CloudPickler(file, protocol\u001b[38;5;241m=\u001b[39mprotocol)\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/mambaforge/envs/scenicgym/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:538\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/ScenicGym/src/scenic/core/scenarios.py:87\u001b[0m, in \u001b[0;36m_Activator.__getstate__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Step 1 (during pickling)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m~/ScenicGym/src/scenic/core/scenarios.py:83\u001b[0m, in \u001b[0;36m_Activator.activate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscenic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msyntax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mveneer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mveneer\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m veneer\u001b[38;5;241m.\u001b[39misActive()\n\u001b[0;32m---> 83\u001b[0m \u001b[43mveneer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompileOptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ScenicGym/src/scenic/syntax/veneer.py:360\u001b[0m, in \u001b[0;36mactivate\u001b[0;34m(options, namespace)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluatingRequirement\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluatingGuard\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m currentSimulation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# placeholder scenario for top-level code\u001b[39;00m\n\u001b[1;32m    362\u001b[0m newScenario \u001b[38;5;241m=\u001b[39m DynamicScenario\u001b[38;5;241m.\u001b[39m_dummy(namespace)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(create_env, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb17aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796a7311",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The environment is of type <class 'scenic.zoo.envs.scenic_zoo.ScenicZooEnv'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m set_random_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_env\u001b[38;5;241m=\u001b[39m\u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m             train_env,\n\u001b[1;32m      5\u001b[0m             n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m      6\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/scenicgym/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:30\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(fn()) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "File \u001b[0;32m~/mambaforge/envs/scenicgym/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [\u001b[43m_patch_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "File \u001b[0;32m~/mambaforge/envs/scenicgym/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:33\u001b[0m, in \u001b[0;36m_patch_env\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gym_installed \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, gym\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(env)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, not a Gymnasium \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment. In this case, we expect OpenAI Gym to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstalled and the environment to be an OpenAI Gym environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshimmy\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The environment is of type <class 'scenic.zoo.envs.scenic_zoo.ScenicZooEnv'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment."
     ]
    }
   ],
   "source": [
    "set_random_seed(0)\n",
    "train_env=DummyVecEnv([partial(create_env, True) for _ in range(4)])\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            train_env,\n",
    "            n_steps=4096,\n",
    "            verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=1000)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d74f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We done\n"
     ]
    }
   ],
   "source": [
    "action = dict(agent0 = [1, 0], agent1=[0, 0])\n",
    "for _ in range(2000):\n",
    "    o, r, te, tc, info = env.step(action)\n",
    "    if te or tc:\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "print(\"We done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceac0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731da071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import contextlib\n",
    "import logging\n",
    "import pathlib\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import scenic\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import tyro\n",
    "from gymnasium import spaces\n",
    "from scenic.gym import ScenicGymEnv\n",
    "from scenic.simulators.metadrive import MetaDriveSimulator\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    \"\"\"Hyperparameters and configuration for the PPO training.\"\"\"\n",
    "\n",
    "    # Environment/scenic file to use\n",
    "    scenic_file: str = \"idm.scenic\"\n",
    "    # Number of parallel processes for data collection\n",
    "    num_workers: int = 4\n",
    "    # Total timesteps for training\n",
    "    total_timesteps: int = 1_000_000\n",
    "    # Timesteps collected by each worker per iteration\n",
    "    steps_per_worker: int = 256\n",
    "    # Number of optimization epochs per PPO iteration\n",
    "    num_epochs: int = 4\n",
    "    # Size of minibatches for optimization\n",
    "    minibatch_size: int = 6\n",
    "    # Discount factor\n",
    "    gamma: float = 0.9\n",
    "    # Lambda for Generalized Advantage Estimation\n",
    "    gae_lambda: float = 0.9\n",
    "    # PPO clipping parameter\n",
    "    clip_epsilon: float = 0.2\n",
    "    # Learning rate\n",
    "    lr: float = 3e-4\n",
    "    # Entropy coefficient for exploration bonus\n",
    "    entropy_coef: float = 0.01\n",
    "    # Value function loss coefficient\n",
    "    value_loss_coef: float = 0.5\n",
    "    # Gradient clipping threshold\n",
    "    max_grad_norm: float = 0.5\n",
    "    # Random seed\n",
    "    seed: int = 4\n",
    "    # Directory to save models\n",
    "    model_dir: str = \"models\"\n",
    "\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "EPSILON = 1e-5\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"A simple Actor-Critic network for discrete action spaces. Shares layers between actor and critic.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, action_space: spaces.Box, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(hidden_dim, np.prod(action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(hidden_dim, np.prod(action_space.shape))\n",
    "        self.critic = nn.Linear(hidden_dim, 1)  # Value head\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"action_scale\",\n",
    "            torch.tensor((action_space.high - action_space.low) / 2.0, dtype=torch.float32),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\",\n",
    "            torch.tensor((action_space.high + action_space.low) / 2.0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: any) -> tuple:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        shared_features = self.shared_layer(x)\n",
    "        mean = self.fc_mean(shared_features)\n",
    "        log_std = self.fc_logstd(shared_features)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)\n",
    "        value = self.critic(shared_features)\n",
    "        return mean, log_std, value\n",
    "\n",
    "\n",
    "# %%\n",
    "def worker_fn(worker_id: int, steps_per_worker: int, model_state_dict: dict, data_queue: mp.Queue, seed: int, scenic_file: str) -> None:\n",
    "    \"\"\"Execute function for each worker process. Initializes environment and model, collects trajectories, and sends data back.\"\"\"\n",
    "    logging.getLogger(__name__).debug(\"Worker %s: Initializing...\", worker_id)\n",
    "    scenario = scenic.scenarioFromFile(\n",
    "        scenic_file,\n",
    "        model=\"scenic.simulators.metadrive.model\",\n",
    "        mode2D=True,\n",
    "    )\n",
    "    env = ScenicGymEnv(\n",
    "        scenario,\n",
    "        MetaDriveSimulator(timestep=0.05, sumo_map=pathlib.Path(\"../maps/Town06.net.xml\"), render=False, real_time=False),\n",
    "        observation_space=spaces.Box(low=-np.inf, high=np.inf, shape=(5, 7)),\n",
    "        action_space=spaces.Box(low=-1, high=1, shape=(2,)),\n",
    "        max_steps=700,\n",
    "    )\n",
    "    obs_space_shape = env.observation_space.shape\n",
    "    action_space = env.action_space\n",
    "    obs_dim = np.prod(obs_space_shape) if isinstance(obs_space_shape, tuple) else obs_space_shape[0]\n",
    "    worker_seed = seed + worker_id\n",
    "    env.reset(seed=worker_seed)\n",
    "\n",
    "    local_model = ActorCritic(obs_dim, action_space)\n",
    "    local_model.load_state_dict(model_state_dict)\n",
    "    local_model.eval()\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    current_step = 0\n",
    "    while current_step < steps_per_worker:\n",
    "        obs_tensor = torch.tensor(obs.flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            mean, log_std, value = local_model(obs_tensor)\n",
    "            std = log_std.exp()\n",
    "            normal = torch.distributions.Normal(mean, std)\n",
    "            x_t = normal.rsample()\n",
    "            y_t = torch.tanh(x_t)\n",
    "            action = y_t * local_model.action_scale + local_model.action_bias\n",
    "            log_prob = normal.log_prob(x_t)\n",
    "            log_prob -= torch.log(local_model.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "            log_prob = log_prob.sum(1, keepdim=True)\n",
    "        action = action.cpu().numpy().squeeze(0)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        observations.append(obs.flatten())\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob.item())\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        values.append(value.item())\n",
    "\n",
    "        obs = next_obs\n",
    "        current_step += 1\n",
    "\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    last_obs_tensor = torch.tensor(obs.flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        _, _, last_value = local_model(last_obs_tensor)\n",
    "        last_value = last_value.item()\n",
    "\n",
    "    trajectory_data = {\n",
    "        \"observations\": np.array(observations, dtype=np.float32),\n",
    "        \"actions\": np.array(actions, dtype=np.float32),\n",
    "        \"log_probs\": np.array(log_probs, dtype=np.float32),\n",
    "        \"rewards\": np.array(rewards, dtype=np.float32),\n",
    "        \"dones\": np.array(dones, dtype=np.bool_),\n",
    "        \"values\": np.array(values, dtype=np.float32),\n",
    "        \"last_value\": last_value,\n",
    "        \"last_done\": done,\n",
    "    }\n",
    "\n",
    "    data_queue.put(trajectory_data)\n",
    "    logging.getLogger(__name__).debug(\"Worker %s: Finished collecting %s steps.\", worker_id, current_step)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# %%\n",
    "def compute_gae(\n",
    "    rewards: np.array,\n",
    "    values: np.array,\n",
    "    dones: np.array,\n",
    "    last_value: float,\n",
    "    last_done: float,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> tuple:\n",
    "    \"\"\"Compute Generalized Advantage Estimation (GAE).\"\"\"\n",
    "    advantages = np.zeros_like(rewards)\n",
    "    last_gae_lam = 0\n",
    "    num_steps = len(rewards)\n",
    "    next_values = np.append(values[1:], last_value if not last_done else 0.0)\n",
    "    next_non_terminal = 1.0 - dones\n",
    "    deltas = rewards + gamma * next_values * next_non_terminal - values\n",
    "\n",
    "    for t in reversed(range(num_steps)):\n",
    "        last_gae_lam = deltas[t] + gamma * gae_lambda * next_non_terminal[t] * last_gae_lam\n",
    "        advantages[t] = last_gae_lam\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# %%\n",
    "def ppo_update(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    batch_obs: torch.Tensor,\n",
    "    batch_actions: torch.Tensor,\n",
    "    batch_log_probs_old: torch.Tensor,\n",
    "    batch_advantages: torch.Tensor,\n",
    "    batch_returns: torch.Tensor,\n",
    "    num_epochs: int,\n",
    "    minibatch_size: int,\n",
    "    clip_epsilon: float,\n",
    "    entropy_coef: float,\n",
    "    value_loss_coef: float,\n",
    "    max_grad_norm: float,\n",
    "    rng: np.random.Generator,\n",
    ") -> None:\n",
    "    \"\"\"Perform the PPO update step using collected batch data.\"\"\"\n",
    "    batch_size = batch_obs.size(0)\n",
    "    batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        indices = rng.permutation(batch_size)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            minibatch_indices = indices[start:end]\n",
    "\n",
    "            mb_obs = batch_obs[minibatch_indices]\n",
    "            mb_actions = batch_actions[minibatch_indices]\n",
    "            mb_log_probs_old = batch_log_probs_old[minibatch_indices]\n",
    "            mb_advantages = batch_advantages[minibatch_indices]\n",
    "            mb_returns = batch_returns[minibatch_indices]\n",
    "\n",
    "            mean, log_std, values_pred = model(mb_obs)\n",
    "            std = log_std.exp()\n",
    "            normal = torch.distributions.Normal(mean, std)\n",
    "\n",
    "            mb_actions_clamped = torch.clamp(mb_actions, -1.0 + EPSILON, 1.0 - EPSILON)\n",
    "            unsquashed_mb_actions = torch.atanh(mb_actions_clamped)\n",
    "            log_probs_gaussian = normal.log_prob(unsquashed_mb_actions).sum(dim=-1)\n",
    "            log_prob_squash_correction = torch.log(1.0 - mb_actions.pow(2) + EPSILON).sum(dim=-1)\n",
    "            log_probs_new = log_probs_gaussian - log_prob_squash_correction\n",
    "\n",
    "            entropy = normal.entropy().mean()\n",
    "            values_pred = values_pred.squeeze(-1)\n",
    "\n",
    "            prob_ratio = torch.exp(log_probs_new - mb_log_probs_old)\n",
    "            surr1 = prob_ratio * mb_advantages\n",
    "            surr2 = torch.clamp(prob_ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * mb_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = 0.5 * ((values_pred - mb_returns) ** 2).mean()\n",
    "\n",
    "            loss = policy_loss + value_loss_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Run the PPO training.\"\"\"\n",
    "    args = tyro.cli(Args)\n",
    "\n",
    "    # Ensure model directory exists\n",
    "    if not pathlib.Path.exists(pathlib.Path(args.model_dir)):\n",
    "        pathlib.Path.mkdir(pathlib.Path(args.model_dir))\n",
    "    print(\"Model directory:\", args.model_dir)\n",
    "\n",
    "    # Set the environment name based on the scenic file\n",
    "    env_name = pathlib.Path(args.scenic_file).stem\n",
    "\n",
    "    # Set up multiprocess start method\n",
    "    with contextlib.suppress(RuntimeError):\n",
    "        mp.set_start_method(\"spawn\")\n",
    "\n",
    "    # seeds\n",
    "    rng = np.random.default_rng(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logger.info(\"Starting PPO training...\")\n",
    "    logger.info(\"Environment: %s, Workers: %s, Total Timesteps: %s\", env_name, args.num_workers, args.total_timesteps)\n",
    "    logger.info(\"Hyperparameters: gamma=%s, lambda=%s, clip_eps=%s, lr=%s\", args.gamma, args.gae_lambda, args.clip_epsilon, args.lr)\n",
    "\n",
    "    # temp env to get obs and action space\n",
    "    env = ScenicGymEnv(\n",
    "        env_name,\n",
    "        MetaDriveSimulator(timestep=0.05, sumo_map=pathlib.Path(\"../maps/Town06.net.xml\"), render=False, real_time=False),\n",
    "        observation_space=spaces.Box(low=-np.inf, high=np.inf, shape=(5, 7)),\n",
    "        action_space=spaces.Box(low=-1, high=1, shape=(2,)),\n",
    "        max_steps=700,\n",
    "    )\n",
    "    obs_space_shape = env.observation_space.shape\n",
    "    action_space = env.action_space\n",
    "    obs_dim = np.prod(obs_space_shape) if isinstance(obs_space_shape, tuple) else obs_space_shape[0]\n",
    "    env.close()\n",
    "\n",
    "    model = ActorCritic(obs_dim, action_space).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    batch_size = args.num_workers * args.steps_per_worker\n",
    "    num_updates = args.total_timesteps // batch_size\n",
    "    logger.info(\"Batch Size (Workers * Steps): %s\", batch_size)\n",
    "    logger.info(\"Total PPO Updates: %s\", num_updates)\n",
    "\n",
    "    data_queue = mp.Queue()\n",
    "\n",
    "    total_steps = 0\n",
    "    start_time = time.time()\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "    episode_lengths = deque(maxlen=100)\n",
    "    total_episodes = 0\n",
    "\n",
    "    # Training Loop\n",
    "    for update in range(1, num_updates + 1):\n",
    "        update_start_time = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        processes = []\n",
    "        current_model_state_dict = model.state_dict()\n",
    "        for i in range(args.num_workers):\n",
    "            p = mp.Process(\n",
    "                target=worker_fn,\n",
    "                args=(\n",
    "                    i,\n",
    "                    args.steps_per_worker,\n",
    "                    current_model_state_dict,\n",
    "                    data_queue,\n",
    "                    args.seed + update * args.num_workers,\n",
    "                    args.scenic_file,\n",
    "                ),\n",
    "            )\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "\n",
    "        all_trajectory_data = [data_queue.get() for _ in range(args.num_workers)]\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        logger.debug(\"Update %s: All workers finished.\", update)\n",
    "\n",
    "        batch_obs_list = []\n",
    "        batch_actions_list = []\n",
    "        batch_log_probs_list = []\n",
    "        batch_advantages_list = []\n",
    "        batch_returns_list = []\n",
    "\n",
    "        for data in all_trajectory_data:\n",
    "            advantages, returns = compute_gae(\n",
    "                data[\"rewards\"],\n",
    "                data[\"values\"],\n",
    "                data[\"dones\"],\n",
    "                data[\"last_value\"],\n",
    "                data[\"last_done\"],\n",
    "                args.gamma,\n",
    "                args.gae_lambda,\n",
    "            )\n",
    "            batch_advantages_list.append(advantages)\n",
    "            batch_returns_list.append(returns)\n",
    "            batch_obs_list.append(data[\"observations\"])\n",
    "            batch_actions_list.append(data[\"actions\"])\n",
    "            batch_log_probs_list.append(data[\"log_probs\"])\n",
    "\n",
    "            current_episode_reward = 0\n",
    "            current_episode_length = 0\n",
    "            for reward, done in zip(data[\"rewards\"], data[\"dones\"], strict=False):\n",
    "                current_episode_reward += reward\n",
    "                current_episode_length += 1\n",
    "                if done:\n",
    "                    episode_rewards.append(current_episode_reward)\n",
    "                    episode_lengths.append(current_episode_length)\n",
    "                    total_episodes += 1\n",
    "                    current_episode_reward = 0\n",
    "                    current_episode_length = 0\n",
    "\n",
    "        batch_obs = torch.tensor(np.concatenate(batch_obs_list), dtype=torch.float32).to(device)\n",
    "        batch_actions = torch.tensor(np.concatenate(batch_actions_list), dtype=torch.float32).to(device)\n",
    "        batch_log_probs_old = torch.tensor(np.concatenate(batch_log_probs_list), dtype=torch.float32).to(device)\n",
    "        batch_advantages = torch.tensor(np.concatenate(batch_advantages_list), dtype=torch.float32).to(device)\n",
    "        batch_returns = torch.tensor(np.concatenate(batch_returns_list), dtype=torch.float32).to(device)\n",
    "\n",
    "        model.train()\n",
    "        ppo_update(\n",
    "            model,\n",
    "            optimizer,\n",
    "            batch_obs,\n",
    "            batch_actions,\n",
    "            batch_log_probs_old,\n",
    "            batch_advantages,\n",
    "            batch_returns,\n",
    "            args.num_epochs,\n",
    "            args.minibatch_size,\n",
    "            args.clip_epsilon,\n",
    "            args.entropy_coef,\n",
    "            args.value_loss_coef,\n",
    "            args.max_grad_norm,\n",
    "            rng,\n",
    "        )\n",
    "\n",
    "        total_steps += batch_size\n",
    "        update_end_time = time.time()\n",
    "        fps = int(batch_size / (update_end_time - update_start_time))\n",
    "        avg_reward = np.mean(episode_rewards) if episode_rewards else 0\n",
    "        avg_length = np.mean(episode_lengths) if episode_lengths else 0\n",
    "\n",
    "        if update % 1 == 0 or update == 1:\n",
    "            logger.info(\n",
    "                \"Update: %s/%s, Timesteps: %s/%s, FPS: %s, Episodes: %s, Avg Reward (Last 100): %.2f, Avg Length (Last 100): %.2f\",\n",
    "                update,\n",
    "                num_updates,\n",
    "                total_steps,\n",
    "                args.total_timesteps,\n",
    "                fps,\n",
    "                total_episodes,\n",
    "                avg_reward,\n",
    "                avg_length,\n",
    "            )\n",
    "            # Save model every 10 updates\n",
    "            torch.save(model.state_dict(), f\"{args.model_dir}/ppo_{env_name}_model.pth\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logger.info(\"Training finished in %.2f seconds.\", end_time - start_time)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{args.model_dir}/ppo_{env_name}_model.pth\")\n",
    "    logger.info(\"Model saved to ppo_%s_model.pth\", env_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
