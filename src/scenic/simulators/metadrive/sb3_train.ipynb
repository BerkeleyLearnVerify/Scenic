{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931f5618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kxu/ScenicGym/src/scenic/core/errors.py:271: UserWarning: unable to install sys.excepthook to format Scenic backtraces\n",
      "  warnings.warn(\"unable to install sys.excepthook to format Scenic backtraces\")\n"
     ]
    }
   ],
   "source": [
    "from scenic.zoo import ScenicZooEnv\n",
    "from scenic.simulators.metadrive import MetaDriveSimulator\n",
    "import scenic\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.ppo import MlpPolicy\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# from stable_baselines3.common.utils import set_random_seed\n",
    "# import supersuit as ss\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12b34e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kxu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_user = os.path.expanduser(\"~\")\n",
    "root_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca76cee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root_user' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sumo_map \u001b[38;5;241m=\u001b[39m \u001b[43mroot_user\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ScenicGym/assets/maps/CARLA/Town01.net.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m obs_space_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent0\u001b[39m\u001b[38;5;124m\"\u001b[39m :  gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m , (\u001b[38;5;241m249\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m      3\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent1\u001b[39m\u001b[38;5;124m\"\u001b[39m: gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m , (\u001b[38;5;241m249\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)}\n\u001b[1;32m      5\u001b[0m action_space_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent0\u001b[39m\u001b[38;5;124m'\u001b[39m: gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, (\u001b[38;5;241m2\u001b[39m,), np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m      6\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent1\u001b[39m\u001b[38;5;124m'\u001b[39m: gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, (\u001b[38;5;241m2\u001b[39m,), np\u001b[38;5;241m.\u001b[39mfloat32)}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root_user' is not defined"
     ]
    }
   ],
   "source": [
    "sumo_map = root_user + \"/ScenicGym/assets/maps/CARLA/Town01.net.xml\"\n",
    "obs_space_dict = {\"agent0\" :  gym.spaces.Box(-0.0, 1.0 , (249,), dtype=np.float32),\n",
    "                 \"agent1\": gym.spaces.Box(-0.0, 1.0 , (249,), dtype=np.float32)}\n",
    "\n",
    "action_space_dict = {'agent0': gym.spaces.Box(-1.0, 1.0, (2,), np.float32),\n",
    "                     'agent1': gym.spaces.Box(-1.0, 1.0, (2,), np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0ff89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = scenic.scenarioFromFile(\"exp.scenic\",\n",
    "                               model=\"scenic.simulators.metadrive.model\",\n",
    "                           mode2D=True)\n",
    "env = ScenicZooEnv(scenario, \n",
    "                       MetaDriveSimulator(sumo_map=sumo_map, render=True, real_time=True),\n",
    "                       None, \n",
    "                       max_steps=50, \n",
    "                       observation_space = obs_space_dict, \n",
    "                       action_space = action_space_dict, \n",
    "                       agents=[\"agent0\", \"agent1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d74f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "actions: {'agent0': [1, 0], 'agent1': [0, 0]}\n",
      "We done\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = dict(agent0 = [1, 0], agent1=[0, 0]) # First is throttle, second is steering\n",
    "for _ in range(2000):\n",
    "    o, r, te, tc, info = env.step(action)\n",
    "    if te or tc:\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "print(\"We done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d817e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_env(need_monitor=False):\n",
    "#     scenario = scenic.scenarioFromFile(\"intersect_drive.scenic\",\n",
    "#                                    model=\"scenic.simulators.metadrive.model\",\n",
    "#                                mode2D=True)\n",
    "\n",
    "#     env = ScenicZooEnv(scenario, \n",
    "#                        MetaDriveSimulator(sumo_map=sumo_map, render=False, real_time=False),\n",
    "#                        None, \n",
    "#                        max_steps=50, \n",
    "#                        observation_space = obs_space_dict, \n",
    "#                        action_space = action_space_dict, \n",
    "#                        agents=[\"agent0\", \"agent1\"])\n",
    "#     #if need_monitor:\n",
    "#     #    env = Monitor(env)\n",
    "#     return env\n",
    "\n",
    "# def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
    "#     # Train a single model to play as each agent in an AEC environment\n",
    "#     env = env_fn()\n",
    "\n",
    "#     # Add black death wrapper so the number of agents stays constant\n",
    "#     # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
    "#     env = ss.black_death_v3(env)\n",
    "\n",
    "#     # Pre-process using SuperSuit\n",
    "# #     visual_observation = not env.unwrapped.vector_state\n",
    "# #     if visual_observation:\n",
    "# #         # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "# #         env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "# #         env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "# #         env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "#     env.reset(seed=seed)\n",
    "\n",
    "#     print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "#     env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "#     env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "#     # Use a CNN policy if the observation space is visual\n",
    "#     model = PPO(\n",
    "#         CnnPolicy if visual_observation else MlpPolicy,\n",
    "#         env,\n",
    "#         verbose=3,\n",
    "#         batch_size=256,\n",
    "#     )\n",
    "\n",
    "#     model.learn(total_timesteps=steps)\n",
    "\n",
    "#     model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "#     print(\"Model has been saved.\")\n",
    "\n",
    "#     print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af408e84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train(create_env, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03161f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796a7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_random_seed(0)\n",
    "# train_env=DummyVecEnv([partial(create_env, True) for _ in range(4)])\n",
    "# model = PPO(\"MlpPolicy\", \n",
    "#             train_env,\n",
    "#             n_steps=4096,\n",
    "#             verbose=1)\n",
    "\n",
    "# model.learn(total_timesteps=1000)\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdeecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731da071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import contextlib\n",
    "import logging\n",
    "import pathlib\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import scenic\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import tyro\n",
    "from gymnasium import spaces\n",
    "from scenic.gym import ScenicGymEnv\n",
    "from scenic.simulators.metadrive import MetaDriveSimulator\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    \"\"\"Hyperparameters and configuration for the PPO training.\"\"\"\n",
    "\n",
    "    # Environment/scenic file to use\n",
    "    scenic_file: str = \"idm.scenic\"\n",
    "    # Number of parallel processes for data collection\n",
    "    num_workers: int = 4\n",
    "    # Total timesteps for training\n",
    "    total_timesteps: int = 1_000_000\n",
    "    # Timesteps collected by each worker per iteration\n",
    "    steps_per_worker: int = 256\n",
    "    # Number of optimization epochs per PPO iteration\n",
    "    num_epochs: int = 4\n",
    "    # Size of minibatches for optimization\n",
    "    minibatch_size: int = 6\n",
    "    # Discount factor\n",
    "    gamma: float = 0.9\n",
    "    # Lambda for Generalized Advantage Estimation\n",
    "    gae_lambda: float = 0.9\n",
    "    # PPO clipping parameter\n",
    "    clip_epsilon: float = 0.2\n",
    "    # Learning rate\n",
    "    lr: float = 3e-4\n",
    "    # Entropy coefficient for exploration bonus\n",
    "    entropy_coef: float = 0.01\n",
    "    # Value function loss coefficient\n",
    "    value_loss_coef: float = 0.5\n",
    "    # Gradient clipping threshold\n",
    "    max_grad_norm: float = 0.5\n",
    "    # Random seed\n",
    "    seed: int = 4\n",
    "    # Directory to save models\n",
    "    model_dir: str = \"models\"\n",
    "\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "EPSILON = 1e-5\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"A simple Actor-Critic network for discrete action spaces. Shares layers between actor and critic.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, action_space: spaces.Box, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(hidden_dim, np.prod(action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(hidden_dim, np.prod(action_space.shape))\n",
    "        self.critic = nn.Linear(hidden_dim, 1)  # Value head\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"action_scale\",\n",
    "            torch.tensor((action_space.high - action_space.low) / 2.0, dtype=torch.float32),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\",\n",
    "            torch.tensor((action_space.high + action_space.low) / 2.0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: any) -> tuple:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        shared_features = self.shared_layer(x)\n",
    "        mean = self.fc_mean(shared_features)\n",
    "        log_std = self.fc_logstd(shared_features)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)\n",
    "        value = self.critic(shared_features)\n",
    "        return mean, log_std, value\n",
    "\n",
    "\n",
    "# %%\n",
    "def worker_fn(worker_id: int, steps_per_worker: int, model_state_dict: dict, data_queue: mp.Queue, seed: int, scenic_file: str) -> None:\n",
    "    \"\"\"Execute function for each worker process. Initializes environment and model, collects trajectories, and sends data back.\"\"\"\n",
    "    logging.getLogger(__name__).debug(\"Worker %s: Initializing...\", worker_id)\n",
    "    scenario = scenic.scenarioFromFile(\n",
    "        scenic_file,\n",
    "        model=\"scenic.simulators.metadrive.model\",\n",
    "        mode2D=True,\n",
    "    )\n",
    "    env = ScenicGymEnv(\n",
    "        scenario,\n",
    "        MetaDriveSimulator(timestep=0.05, sumo_map=pathlib.Path(\"../maps/Town06.net.xml\"), render=False, real_time=False),\n",
    "        observation_space=spaces.Box(low=-np.inf, high=np.inf, shape=(5, 7)),\n",
    "        action_space=spaces.Box(low=-1, high=1, shape=(2,)),\n",
    "        max_steps=700,\n",
    "    )\n",
    "    obs_space_shape = env.observation_space.shape\n",
    "    action_space = env.action_space\n",
    "    obs_dim = np.prod(obs_space_shape) if isinstance(obs_space_shape, tuple) else obs_space_shape[0]\n",
    "    worker_seed = seed + worker_id\n",
    "    env.reset(seed=worker_seed)\n",
    "\n",
    "    local_model = ActorCritic(obs_dim, action_space)\n",
    "    local_model.load_state_dict(model_state_dict)\n",
    "    local_model.eval()\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    current_step = 0\n",
    "    while current_step < steps_per_worker:\n",
    "        obs_tensor = torch.tensor(obs.flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            mean, log_std, value = local_model(obs_tensor)\n",
    "            std = log_std.exp()\n",
    "            normal = torch.distributions.Normal(mean, std)\n",
    "            x_t = normal.rsample()\n",
    "            y_t = torch.tanh(x_t)\n",
    "            action = y_t * local_model.action_scale + local_model.action_bias\n",
    "            log_prob = normal.log_prob(x_t)\n",
    "            log_prob -= torch.log(local_model.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "            log_prob = log_prob.sum(1, keepdim=True)\n",
    "        action = action.cpu().numpy().squeeze(0)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        observations.append(obs.flatten())\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob.item())\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        values.append(value.item())\n",
    "\n",
    "        obs = next_obs\n",
    "        current_step += 1\n",
    "\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    last_obs_tensor = torch.tensor(obs.flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        _, _, last_value = local_model(last_obs_tensor)\n",
    "        last_value = last_value.item()\n",
    "\n",
    "    trajectory_data = {\n",
    "        \"observations\": np.array(observations, dtype=np.float32),\n",
    "        \"actions\": np.array(actions, dtype=np.float32),\n",
    "        \"log_probs\": np.array(log_probs, dtype=np.float32),\n",
    "        \"rewards\": np.array(rewards, dtype=np.float32),\n",
    "        \"dones\": np.array(dones, dtype=np.bool_),\n",
    "        \"values\": np.array(values, dtype=np.float32),\n",
    "        \"last_value\": last_value,\n",
    "        \"last_done\": done,\n",
    "    }\n",
    "\n",
    "    data_queue.put(trajectory_data)\n",
    "    logging.getLogger(__name__).debug(\"Worker %s: Finished collecting %s steps.\", worker_id, current_step)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# %%\n",
    "def compute_gae(\n",
    "    rewards: np.array,\n",
    "    values: np.array,\n",
    "    dones: np.array,\n",
    "    last_value: float,\n",
    "    last_done: float,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> tuple:\n",
    "    \"\"\"Compute Generalized Advantage Estimation (GAE).\"\"\"\n",
    "    advantages = np.zeros_like(rewards)\n",
    "    last_gae_lam = 0\n",
    "    num_steps = len(rewards)\n",
    "    next_values = np.append(values[1:], last_value if not last_done else 0.0)\n",
    "    next_non_terminal = 1.0 - dones\n",
    "    deltas = rewards + gamma * next_values * next_non_terminal - values\n",
    "\n",
    "    for t in reversed(range(num_steps)):\n",
    "        last_gae_lam = deltas[t] + gamma * gae_lambda * next_non_terminal[t] * last_gae_lam\n",
    "        advantages[t] = last_gae_lam\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# %%\n",
    "def ppo_update(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    batch_obs: torch.Tensor,\n",
    "    batch_actions: torch.Tensor,\n",
    "    batch_log_probs_old: torch.Tensor,\n",
    "    batch_advantages: torch.Tensor,\n",
    "    batch_returns: torch.Tensor,\n",
    "    num_epochs: int,\n",
    "    minibatch_size: int,\n",
    "    clip_epsilon: float,\n",
    "    entropy_coef: float,\n",
    "    value_loss_coef: float,\n",
    "    max_grad_norm: float,\n",
    "    rng: np.random.Generator,\n",
    ") -> None:\n",
    "    \"\"\"Perform the PPO update step using collected batch data.\"\"\"\n",
    "    batch_size = batch_obs.size(0)\n",
    "    batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        indices = rng.permutation(batch_size)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            minibatch_indices = indices[start:end]\n",
    "\n",
    "            mb_obs = batch_obs[minibatch_indices]\n",
    "            mb_actions = batch_actions[minibatch_indices]\n",
    "            mb_log_probs_old = batch_log_probs_old[minibatch_indices]\n",
    "            mb_advantages = batch_advantages[minibatch_indices]\n",
    "            mb_returns = batch_returns[minibatch_indices]\n",
    "\n",
    "            mean, log_std, values_pred = model(mb_obs)\n",
    "            std = log_std.exp()\n",
    "            normal = torch.distributions.Normal(mean, std)\n",
    "\n",
    "            mb_actions_clamped = torch.clamp(mb_actions, -1.0 + EPSILON, 1.0 - EPSILON)\n",
    "            unsquashed_mb_actions = torch.atanh(mb_actions_clamped)\n",
    "            log_probs_gaussian = normal.log_prob(unsquashed_mb_actions).sum(dim=-1)\n",
    "            log_prob_squash_correction = torch.log(1.0 - mb_actions.pow(2) + EPSILON).sum(dim=-1)\n",
    "            log_probs_new = log_probs_gaussian - log_prob_squash_correction\n",
    "\n",
    "            entropy = normal.entropy().mean()\n",
    "            values_pred = values_pred.squeeze(-1)\n",
    "\n",
    "            prob_ratio = torch.exp(log_probs_new - mb_log_probs_old)\n",
    "            surr1 = prob_ratio * mb_advantages\n",
    "            surr2 = torch.clamp(prob_ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * mb_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = 0.5 * ((values_pred - mb_returns) ** 2).mean()\n",
    "\n",
    "            loss = policy_loss + value_loss_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Run the PPO training.\"\"\"\n",
    "    args = tyro.cli(Args)\n",
    "\n",
    "    # Ensure model directory exists\n",
    "    if not pathlib.Path.exists(pathlib.Path(args.model_dir)):\n",
    "        pathlib.Path.mkdir(pathlib.Path(args.model_dir))\n",
    "    print(\"Model directory:\", args.model_dir)\n",
    "\n",
    "    # Set the environment name based on the scenic file\n",
    "    env_name = pathlib.Path(args.scenic_file).stem\n",
    "\n",
    "    # Set up multiprocess start method\n",
    "    with contextlib.suppress(RuntimeError):\n",
    "        mp.set_start_method(\"spawn\")\n",
    "\n",
    "    # seeds\n",
    "    rng = np.random.default_rng(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logger.info(\"Starting PPO training...\")\n",
    "    logger.info(\"Environment: %s, Workers: %s, Total Timesteps: %s\", env_name, args.num_workers, args.total_timesteps)\n",
    "    logger.info(\"Hyperparameters: gamma=%s, lambda=%s, clip_eps=%s, lr=%s\", args.gamma, args.gae_lambda, args.clip_epsilon, args.lr)\n",
    "\n",
    "    # temp env to get obs and action space\n",
    "    env = ScenicGymEnv(\n",
    "        env_name,\n",
    "        MetaDriveSimulator(timestep=0.05, sumo_map=pathlib.Path(\"../maps/Town06.net.xml\"), render=False, real_time=False),\n",
    "        observation_space=spaces.Box(low=-np.inf, high=np.inf, shape=(5, 7)),\n",
    "        action_space=spaces.Box(low=-1, high=1, shape=(2,)),\n",
    "        max_steps=700,\n",
    "    )\n",
    "    obs_space_shape = env.observation_space.shape\n",
    "    action_space = env.action_space\n",
    "    obs_dim = np.prod(obs_space_shape) if isinstance(obs_space_shape, tuple) else obs_space_shape[0]\n",
    "    env.close()\n",
    "\n",
    "    model = ActorCritic(obs_dim, action_space).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    batch_size = args.num_workers * args.steps_per_worker\n",
    "    num_updates = args.total_timesteps // batch_size\n",
    "    logger.info(\"Batch Size (Workers * Steps): %s\", batch_size)\n",
    "    logger.info(\"Total PPO Updates: %s\", num_updates)\n",
    "\n",
    "    data_queue = mp.Queue()\n",
    "\n",
    "    total_steps = 0\n",
    "    start_time = time.time()\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "    episode_lengths = deque(maxlen=100)\n",
    "    total_episodes = 0\n",
    "\n",
    "    # Training Loop\n",
    "    for update in range(1, num_updates + 1):\n",
    "        update_start_time = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        processes = []\n",
    "        current_model_state_dict = model.state_dict()\n",
    "        for i in range(args.num_workers):\n",
    "            p = mp.Process(\n",
    "                target=worker_fn,\n",
    "                args=(\n",
    "                    i,\n",
    "                    args.steps_per_worker,\n",
    "                    current_model_state_dict,\n",
    "                    data_queue,\n",
    "                    args.seed + update * args.num_workers,\n",
    "                    args.scenic_file,\n",
    "                ),\n",
    "            )\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "\n",
    "        all_trajectory_data = [data_queue.get() for _ in range(args.num_workers)]\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        logger.debug(\"Update %s: All workers finished.\", update)\n",
    "\n",
    "        batch_obs_list = []\n",
    "        batch_actions_list = []\n",
    "        batch_log_probs_list = []\n",
    "        batch_advantages_list = []\n",
    "        batch_returns_list = []\n",
    "\n",
    "        for data in all_trajectory_data:\n",
    "            advantages, returns = compute_gae(\n",
    "                data[\"rewards\"],\n",
    "                data[\"values\"],\n",
    "                data[\"dones\"],\n",
    "                data[\"last_value\"],\n",
    "                data[\"last_done\"],\n",
    "                args.gamma,\n",
    "                args.gae_lambda,\n",
    "            )\n",
    "            batch_advantages_list.append(advantages)\n",
    "            batch_returns_list.append(returns)\n",
    "            batch_obs_list.append(data[\"observations\"])\n",
    "            batch_actions_list.append(data[\"actions\"])\n",
    "            batch_log_probs_list.append(data[\"log_probs\"])\n",
    "\n",
    "            current_episode_reward = 0\n",
    "            current_episode_length = 0\n",
    "            for reward, done in zip(data[\"rewards\"], data[\"dones\"], strict=False):\n",
    "                current_episode_reward += reward\n",
    "                current_episode_length += 1\n",
    "                if done:\n",
    "                    episode_rewards.append(current_episode_reward)\n",
    "                    episode_lengths.append(current_episode_length)\n",
    "                    total_episodes += 1\n",
    "                    current_episode_reward = 0\n",
    "                    current_episode_length = 0\n",
    "\n",
    "        batch_obs = torch.tensor(np.concatenate(batch_obs_list), dtype=torch.float32).to(device)\n",
    "        batch_actions = torch.tensor(np.concatenate(batch_actions_list), dtype=torch.float32).to(device)\n",
    "        batch_log_probs_old = torch.tensor(np.concatenate(batch_log_probs_list), dtype=torch.float32).to(device)\n",
    "        batch_advantages = torch.tensor(np.concatenate(batch_advantages_list), dtype=torch.float32).to(device)\n",
    "        batch_returns = torch.tensor(np.concatenate(batch_returns_list), dtype=torch.float32).to(device)\n",
    "\n",
    "        model.train()\n",
    "        ppo_update(\n",
    "            model,\n",
    "            optimizer,\n",
    "            batch_obs,\n",
    "            batch_actions,\n",
    "            batch_log_probs_old,\n",
    "            batch_advantages,\n",
    "            batch_returns,\n",
    "            args.num_epochs,\n",
    "            args.minibatch_size,\n",
    "            args.clip_epsilon,\n",
    "            args.entropy_coef,\n",
    "            args.value_loss_coef,\n",
    "            args.max_grad_norm,\n",
    "            rng,\n",
    "        )\n",
    "\n",
    "        total_steps += batch_size\n",
    "        update_end_time = time.time()\n",
    "        fps = int(batch_size / (update_end_time - update_start_time))\n",
    "        avg_reward = np.mean(episode_rewards) if episode_rewards else 0\n",
    "        avg_length = np.mean(episode_lengths) if episode_lengths else 0\n",
    "\n",
    "        if update % 1 == 0 or update == 1:\n",
    "            logger.info(\n",
    "                \"Update: %s/%s, Timesteps: %s/%s, FPS: %s, Episodes: %s, Avg Reward (Last 100): %.2f, Avg Length (Last 100): %.2f\",\n",
    "                update,\n",
    "                num_updates,\n",
    "                total_steps,\n",
    "                args.total_timesteps,\n",
    "                fps,\n",
    "                total_episodes,\n",
    "                avg_reward,\n",
    "                avg_length,\n",
    "            )\n",
    "            # Save model every 10 updates\n",
    "            torch.save(model.state_dict(), f\"{args.model_dir}/ppo_{env_name}_model.pth\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logger.info(\"Training finished in %.2f seconds.\", end_time - start_time)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{args.model_dir}/ppo_{env_name}_model.pth\")\n",
    "    logger.info(\"Model saved to ppo_%s_model.pth\", env_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
